import asyncio
import json
import os
import time
from typing import Annotated, List, Optional, TypedDict

import chainlit as cl
import pandas as pd
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import END, StateGraph
from pydantic import BaseModel, Field

# --- ç¯å¢ƒå˜é‡é…ç½® ---
# è¯·ç¡®ä¿åœ¨æ‚¨çš„ç¯å¢ƒä¸­è®¾ç½®äº† OPENAI_API_KEY å’Œ OPENAI_BASE_URL
# ä¾‹å¦‚ï¼Œåœ¨ç»ˆç«¯ä¸­è¿è¡Œ:
# export OPENAI_API_KEY="your_key_here"
# export OPENAI_BASE_URL="your_base_url_here" # å¦‚æœä½¿ç”¨ä»£ç†

# --- 1. å®šä¹‰LLMå’Œé‡è¯•é…ç½® ---
MAX_RETRIES = 5  # æœ€å¤§é‡è¯•æ¬¡æ•°

# åˆå§‹åŒ–LLMæ¨¡å‹
# temperature=0 è¡¨ç¤ºæˆ‘ä»¬å¸Œæœ›LLMçš„è¾“å‡ºæ›´å…·ç¡®å®šæ€§
llm = ChatOpenAI(
    base_url=os.getenv("SILICONFLOW_BASE_URL"),
    api_key=os.getenv("SILICONFLOW_API_KEY"),
    # å…³é”®å‚æ•°ï¼šé€šè¿‡extra_bodyå…³é—­thinkåŠŸèƒ½
    # model="Qwen/Qwen3-8B",
    # model="qwen3-30b-a3b",
    # model="qwen3-8b",
    # model="Qwen3-30B-A3B",
    # model="DeepSeek-R1-Distill-Qwen-14B-AWQ",
    # model="fiberhome-chat",
    # model="DeepSeek-R1-Distill-Qwen-32B",
    # model="Qwen/Qwen3-8B",
    model="Qwen/Qwen3-30B-A3B",
    # model="Qwen/Qwen3-235B-A22B",
    temperature=0.4,
    # top_p=0.95,
)


# --- 2. æ¨¡æ‹Ÿå¤–éƒ¨æœåŠ¡ ---


def mock_db_query(table_name: str) -> dict:
    """
    æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢ï¼Œè¿”å›è¡¨çš„å…ƒæ•°æ®ã€‚
    åœ¨çœŸå®åœºæ™¯ä¸­ï¼Œè¿™é‡Œä¼šè¿æ¥åˆ°å®é™…çš„æ•°æ®åº“å¹¶æ‰§è¡ŒæŸ¥è¯¢ã€‚
    """
    print(f"--- æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢: {table_name} ---")
    time.sleep(1)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
    schemas = {
        "users": {
            "description": "ç”¨æˆ·ä¿¡æ¯è¡¨",
            "name": "users",  # åœ¨ generate_config_node ä¸­è¢«å¼•ç”¨
            "columns": [
                {
                    "name": "id",
                    "type": "int",
                    "primary_key": True,
                    "description": "ç”¨æˆ·ID",
                },
                {"name": "name", "type": "varchar", "description": "ç”¨æˆ·å§“å"},
                {"name": "age", "type": "int", "description": "ç”¨æˆ·å¹´é¾„"},
                {"name": "email", "type": "varchar", "description": "ç”µå­é‚®ç®±"},
                {"name": "created_at", "type": "datetime", "description": "åˆ›å»ºæ—¶é—´"},
            ],
        },
        "orders": {
            "description": "è®¢å•ä¿¡æ¯è¡¨",
            "name": "orders",  # åœ¨ generate_config_node ä¸­è¢«å¼•ç”¨
            "columns": [
                {
                    "name": "order_id",
                    "type": "int",
                    "primary_key": True,
                    "description": "è®¢å•ID",
                },
                {"name": "user_id", "type": "int", "description": "å…³è”çš„ç”¨æˆ·ID"},
                {"name": "product_name", "type": "varchar", "description": "äº§å“åç§°"},
                {"name": "amount", "type": "decimal", "description": "è®¢å•é‡‘é¢"},
                {"name": "order_date", "type": "date", "description": "ä¸‹å•æ—¥æœŸ"},
            ],
        },
    }
    return schemas.get(table_name.lower(), None)


async def mock_data_engine_service(config_json: dict) -> str:
    """
    æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå¼•æ“æœåŠ¡ã€‚
    æ¥æ”¶JSONé…ç½®ï¼Œæ¨¡æ‹Ÿæ‰§è¡Œæ•°æ®ç”Ÿæˆä»»åŠ¡ï¼Œå¹¶è¿”å›ç»“æœURLã€‚
    """
    print(
        f"--- æ¨¡æ‹Ÿè°ƒç”¨æ•°æ®ç”Ÿæˆå¼•æ“ï¼Œé…ç½®: {json.dumps(config_json, indent=2, ensure_ascii=False)} ---"
    )
    await cl.Message(
        content="âœ… é…ç½®å·²æäº¤è‡³æ•°æ®ç”Ÿæˆå¼•æ“ï¼Œæ­£åœ¨ç”Ÿæˆæ•°æ®... (æ¨¡æ‹Ÿè€—æ—¶3ç§’)"
    ).send()
    await asyncio.sleep(3)
    return "https://example.com/generated_data/result_12345.csv"


# --- 3. LangGraphçŠ¶æ€å®šä¹‰ ---


class StructuredIntent(BaseModel):
    """ç»“æ„åŒ–ç”¨æˆ·æ„å›¾"""

    table_name: str = Field(description="ç”¨æˆ·æƒ³è¦æ“ä½œçš„æ•°æ®åº“è¡¨å")
    constraints: str = Field(description="ç”¨æˆ·å¯¹ç”Ÿæˆæ•°æ®çš„å…·ä½“è¦æ±‚å’Œçº¦æŸæ¡ä»¶")
    num_rows: int = Field(description="æœŸæœ›ç”Ÿæˆçš„æ•°æ®è¡Œæ•°")


class GraphState(TypedDict):
    """
    å®šä¹‰å›¾ï¼ˆGraphï¼‰çš„çŠ¶æ€ã€‚
    æ‰€æœ‰èŠ‚ç‚¹ä¹‹é—´é€šè¿‡è¿™ä¸ªçŠ¶æ€å¯¹è±¡ä¼ é€’æ•°æ®ã€‚
    """

    user_initial_intent: str  # ç”¨æˆ·çš„åŸå§‹è¾“å…¥
    structured_intent: Optional[StructuredIntent]  # LLMè§£æåçš„ç»“æ„åŒ–æ„å›¾
    user_feedback: Optional[str]  # ç”¨æˆ·åœ¨å®¡æ ¸é˜¶æ®µçš„åé¦ˆ
    table_schema: Optional[dict]  # ä»æ•°æ®åº“æŸ¥è¯¢åˆ°çš„è¡¨å…ƒæ•°æ®
    generation_config_json: Optional[dict]  # LLMç”Ÿæˆçš„é…ç½®JSON
    result_url: Optional[str]  # æœ€ç»ˆç”Ÿæˆçš„æ•°æ®ä¸‹è½½é“¾æ¥
    error_message: Optional[str]  # æµç¨‹ä¸­å‘ç”Ÿçš„é”™è¯¯ä¿¡æ¯
    retry_count: int  # å½“å‰é‡è¯•æ¬¡æ•°


# --- 4. LangGraph èŠ‚ç‚¹å®šä¹‰ ---


@cl.step(name="1. æ„å›¾è¯†åˆ« (LLM)", type="llm")
async def intent_recognition_node(state: GraphState):
    """
    ä½¿ç”¨LLMè¯†åˆ«ç”¨æˆ·çš„æ„å›¾ï¼Œå¹¶ç»“æ„åŒ–è¾“å‡ºã€‚
    åŒ…å«é‡è¯•æœºåˆ¶ã€‚
    """
    state["retry_count"] = 0
    state["error_message"] = None
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡è§„åˆ’ä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·è¾“å…¥çš„è‡ªç„¶è¯­è¨€ï¼Œè§£æå‡ºæ ¸å¿ƒçš„ä»»åŠ¡å‚æ•°ã€‚
    ç”¨æˆ·çš„è¾“å…¥æ˜¯ï¼š'{state['user_initial_intent']}'
    ç”¨æˆ·çš„åé¦ˆæ˜¯ï¼š'{state.get('user_feedback', 'æ— ')}'
    è¯·å°†è§£æç»“æœä»¥JSONæ ¼å¼è¾“å‡ºã€‚
    """

    while state["retry_count"] < MAX_RETRIES:
        try:
            structured_llm = llm.with_structured_output(StructuredIntent)
            intent = await structured_llm.ainvoke(prompt)
            state["structured_intent"] = intent
            state["user_feedback"] = None  # æ¸…ç©ºåé¦ˆ
            return state
        except Exception as e:
            state["retry_count"] += 1
            await cl.Message(
                content=f"æ„å›¾è¯†åˆ«å¤±è´¥ï¼Œæ­£åœ¨è¿›è¡Œç¬¬ {state['retry_count']}/{MAX_RETRIES} æ¬¡é‡è¯•... é”™è¯¯: {e}"
            ).send()
            await asyncio.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•

    state["error_message"] = f"æ„å›¾è¯†åˆ«åœ¨é‡è¯•{MAX_RETRIES}æ¬¡åä»ç„¶å¤±è´¥ã€‚"
    return state


@cl.step(name="å®¡æŸ¥æ„å›¾å‚æ•°")
async def intent_review_node(state: GraphState):
    """
    ä¸­æ–­æµç¨‹ï¼Œè®©äººå·¥å®¡æŸ¥LLMæå–çš„å‚æ•°ã€‚
    """
    if state.get("error_message"):
        return state

    intent = state["structured_intent"]
    review_msg = f"""
    è¯·æ‚¨å®¡æŸ¥ä»¥ä¸‹ç”±ç³»ç»Ÿè¯†åˆ«å‡ºçš„ä»»åŠ¡å‚æ•°ï¼š
    - **è¡¨åç§°**: `{intent.table_name}`
    - **çº¦æŸæ¡ä»¶**: `{intent.constraints}`
    - **ç”Ÿæˆæ¡æ•°**: `{intent.num_rows}`

    å¦‚æœå‚æ•°æ­£ç¡®ï¼Œè¯·è¾“å…¥ **æ­£ç¡®**
    å¦‚æœéœ€è¦ä¿®æ”¹ï¼Œè¯·ç›´æ¥è¾“å…¥æ‚¨çš„ä¿®æ”¹æ„è§ã€‚
    """
    res = await cl.AskUserMessage(content=review_msg, timeout=300).send()
    # BUGFIX: ä½¿ç”¨ 'output' é”®è·å–ç”¨æˆ·å›å¤ï¼Œè€Œä¸æ˜¯ 'content'
    if res and res["output"].strip() == "æ­£ç¡®":
        state["user_feedback"] = "æ­£ç¡®"
    else:
        state["user_feedback"] = res["output"] if res else "ç”¨æˆ·æœªæä¾›åé¦ˆã€‚"

    return state


@cl.step(name="2. æŸ¥è¯¢æ•°æ®åº“å…ƒæ•°æ®")
async def query_database_node(state: GraphState):
    """
    æ ¹æ®è¡¨åæŸ¥è¯¢æ•°æ®åº“ï¼Œè·å–è¡¨ç»“æ„ä¿¡æ¯ã€‚
    """
    table_name = state["structured_intent"].table_name
    await cl.Message(
        content=f"å‚æ•°ç¡®è®¤æ— è¯¯ï¼Œæ­£åœ¨æŸ¥è¯¢ `{table_name}` è¡¨çš„å…ƒæ•°æ®..."
    ).send()

    schema = mock_db_query(table_name)
    if schema:
        state["table_schema"] = schema
        # ä½¿ç”¨Pandaså’ŒChainlitå±•ç¤ºè¡¨æ ¼
        df = pd.DataFrame(schema["columns"])
        await cl.Message(
            content=f"æˆåŠŸè·å– **{table_name}** ({schema['description']}) çš„å…ƒæ•°æ®ä¿¡æ¯ï¼š",
            elements=[
                cl.Dataframe(name=f"schema_{table_name}", content=df, size="large")
            ],
        ).send()
    else:
        state["error_message"] = (
            f"æœªåœ¨æ•°æ®åº“ä¸­æ‰¾åˆ°åä¸º '{table_name}' çš„è¡¨ã€‚è¯·æ£€æŸ¥è¡¨åæ˜¯å¦æ­£ç¡®ã€‚"
        )

    return state


@cl.step(name="3. ç”Ÿæˆæ•°æ®å¼•æ“é…ç½® (LLM)", type="llm")
async def generate_config_node(state: GraphState):
    """
    æ ¹æ®è¡¨ç»“æ„å’Œç”¨æˆ·çº¦æŸï¼Œç”Ÿæˆæ•°æ®ç”Ÿæˆå¼•æ“çš„JSONé…ç½®ã€‚
    """
    state["retry_count"] = 0
    state["error_message"] = None

    # æ¸…æ™°åœ°æ„å»ºPrompt
    system_prompt = """
    ä½ æ˜¯ä¸€ä¸ªæ•°æ®ç”Ÿæˆé…ç½®ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·éœ€æ±‚å’Œæ•°æ®åº“è¡¨ç»“æ„ï¼Œä¸ºæ•°æ®ç”Ÿæˆå¼•æ“åˆ›å»ºä¸€ä¸ªè¯¦ç»†çš„JSONé…ç½®æ–‡ä»¶ã€‚

    # è§„åˆ™è¯´æ˜
    1.  **JSONç»“æ„**: é¡¶çº§é”®åº”ä¸º `tableName` å’Œ `rules`ã€‚
    2.  **`rules` å­—æ®µ**: æ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ å¯¹åº”è¡¨ä¸­çš„ä¸€ä¸ªå­—æ®µã€‚
    3.  **å­—æ®µé…ç½®**: æ¯ä¸ªå­—æ®µé…ç½®å¯¹è±¡å¿…é¡»åŒ…å« `fieldName` å’Œ `generator`ã€‚
    4.  **`generator`**: å®šä¹‰äº†æ•°æ®ç”Ÿæˆæ–¹å¼ï¼Œå®ƒæ˜¯ä¸€ä¸ªåŒ…å« `type` å’Œ `params` çš„å¯¹è±¡ã€‚
        - `type`: ç”Ÿæˆå™¨ç±»å‹ (e.g., 'increment', 'random_string', 'random_number', 'enum', 'datetime', 'expression')ã€‚
        - `params`: ç”Ÿæˆå™¨æ‰€éœ€çš„å‚æ•° (e.g., `start`, `length`, `min`, `max`, `values`, `format`, `expr`)ã€‚
    5.  **å­—æ®µå…³è”**: ä½¿ç”¨ `expression` ç±»å‹ç”Ÿæˆå™¨å¯ä»¥å¼•ç”¨å…¶ä»–å­—æ®µçš„å€¼ï¼Œæ ¼å¼ä¸º `f"{fieldName}"`ã€‚

    # ç¤ºä¾‹
    - **è‡ªå¢ID**: `{"fieldName": "id", "generator": {"type": "increment", "params": {"start": 1}}}`
    - **éšæœºå§“å**: `{"fieldName": "name", "generator": {"type": "random_string", "params": {"pattern": "[\u4e00-\u9fa5]{2,3}"}}}` (2åˆ°3ä¸ªä¸­æ–‡å­—ç¬¦)
    - **å…³è”å­—æ®µ**: `{"fieldName": "full_name", "generator": {"type": "expression", "params": {"expr": "f'{first_name}_{last_name}'"}}}`

    è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸Šè§„åˆ™å’Œç”¨æˆ·çš„å…·ä½“è¦æ±‚ã€‚
    """

    user_prompt = f"""
    # ç”¨æˆ·éœ€æ±‚
    - **è¡¨å**: {state['table_schema']['name']}
    - **å­—æ®µçº¦æŸ**: {state['structured_intent'].constraints}
    - **ç”Ÿæˆè¡Œæ•°**: {state['structured_intent'].num_rows}

    # è¡¨ç»“æ„
    {json.dumps(state['table_schema'], indent=2, ensure_ascii=False)}

    # ç”¨æˆ·åé¦ˆ (å¦‚æœ‰)
    {state.get('user_feedback', 'æ— ')}

    è¯·æ ¹æ®ä»¥ä¸Šæ‰€æœ‰ä¿¡æ¯ï¼Œç”Ÿæˆæœ€ç»ˆçš„JSONé…ç½®æ–‡ä»¶ã€‚
    """

    while state["retry_count"] < MAX_RETRIES:
        try:
            # ä½¿ç”¨ .ainvoke è€Œä¸æ˜¯ with_structured_output æ¥è·å–åŸå§‹JSONå­—ç¬¦ä¸²ï¼Œä¾¿äºå±•ç¤º
            response = await llm.ainvoke(
                [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=user_prompt),
                ]
            )
            # æå–å¹¶è§£æJSON
            json_str = (
                response.content.strip().replace("```json", "").replace("```", "")
            )
            config_json = json.loads(json_str)
            state["generation_config_json"] = config_json
            state["user_feedback"] = None  # æ¸…ç©ºåé¦ˆ
            return state
        except Exception as e:
            state["retry_count"] += 1
            await cl.Message(
                content=f"é…ç½®ç”Ÿæˆå¤±è´¥ï¼Œæ­£åœ¨è¿›è¡Œç¬¬ {state['retry_count']}/{MAX_RETRIES} æ¬¡é‡è¯•... é”™è¯¯: {e}"
            ).send()
            await asyncio.sleep(1)

    state["error_message"] = f"é…ç½®ç”Ÿæˆåœ¨é‡è¯•{MAX_RETRIES}æ¬¡åä»ç„¶å¤±è´¥ã€‚"
    return state


@cl.step(name="å®¡æŸ¥JSONé…ç½®")
async def config_review_node(state: GraphState):
    """
    ä¸­æ–­æµç¨‹ï¼Œè®©äººå·¥å®¡æŸ¥ç”Ÿæˆçš„JSONé…ç½®ã€‚
    """
    if state.get("error_message"):
        return state

    config_json = state["generation_config_json"]
    pretty_json = json.dumps(config_json, indent=4, ensure_ascii=False)

    await cl.Message(
        content="ç³»ç»Ÿå·²æ ¹æ®æ‚¨çš„è¦æ±‚ç”Ÿæˆäº†ä»¥ä¸‹æ•°æ®ç”Ÿæˆå¼•æ“é…ç½®ï¼Œè¯·å®¡æŸ¥ï¼š",
        # BUGFIX: ç›´æ¥ä½¿ç”¨å¯¼å…¥çš„ Code ç±»
        elements=[
            Code(content=pretty_json, language="json", title="engine_config.json")
        ],
    ).send()

    review_msg = """
    å¦‚æœé…ç½®æ­£ç¡®ï¼Œè¯·è¾“å…¥ **ç”Ÿæˆ** ä»¥å¼€å§‹æ•°æ®ç”Ÿæˆã€‚
    å¦‚æœéœ€è¦ä¿®æ”¹ï¼Œè¯·ç›´æ¥è¾“å…¥æ‚¨çš„ä¿®æ”¹æ„è§ã€‚
    """
    res = await cl.AskUserMessage(content=review_msg, timeout=300).send()

    # BUGFIX: ä½¿ç”¨ 'output' é”®è·å–ç”¨æˆ·å›å¤ï¼Œè€Œä¸æ˜¯ 'content'
    if res and res["output"].strip() == "ç”Ÿæˆ":
        state["user_feedback"] = "ç”Ÿæˆ"
    else:
        state["user_feedback"] = res["output"] if res else "ç”¨æˆ·æœªæä¾›åé¦ˆã€‚"

    return state


@cl.step(name="4. è°ƒç”¨æ•°æ®ç”Ÿæˆå¼•æ“")
async def post_to_engine_node(state: GraphState):
    """
    å°†é…ç½®POSTåˆ°æ•°æ®ç”Ÿæˆå¼•æ“ï¼Œå¹¶è·å–ç»“æœã€‚
    """
    config_json = state["generation_config_json"]
    result_url = await mock_data_engine_service(config_json)
    state["result_url"] = result_url
    return state


@cl.step(name="5. å±•ç¤ºç»“æœ")
async def show_result_node(state: GraphState):
    """
    åœ¨UIä¸­å±•ç¤ºæœ€ç»ˆçš„ä¸‹è½½é“¾æ¥ã€‚
    """
    url = state["result_url"]
    await cl.Message(
        content=f"ğŸ‰ æ•°æ®ç”ŸæˆæˆåŠŸï¼\n\næ‚¨å¯ä»¥ç‚¹å‡»ä»¥ä¸‹é“¾æ¥ä¸‹è½½ç»“æœï¼š\n[{url}]({url})"
    ).send()
    return state


@cl.step(name="å¤„ç†é”™è¯¯")
async def error_node(state: GraphState):
    """
    å¤„ç†æµç¨‹ä¸­çš„é”™è¯¯ï¼Œå¹¶å‘ç”¨æˆ·æŠ¥å‘Šã€‚
    """
    await cl.Message(
        content=f"âŒ æµç¨‹ä¸­æ–­ï¼Œå‘ç”Ÿé”™è¯¯ï¼š\n\n`{state['error_message']}`"
    ).send()
    return state


# --- 5. LangGraph æ¡ä»¶è¾¹ ---


def should_continue_after_intent_review(state: GraphState):
    if state.get("error_message"):
        return "error"
    if state.get("user_feedback") == "æ­£ç¡®":
        return "continue"
    return "revise_intent"


def should_continue_after_config_review(state: GraphState):
    if state.get("error_message"):
        return "error"
    if state.get("user_feedback") == "ç”Ÿæˆ":
        return "continue"
    return "revise_config"


# --- 6. æ„å»ºLangGraphå·¥ä½œæµ ---

workflow = StateGraph(GraphState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("intent_recognition", intent_recognition_node)
workflow.add_node("intent_review", intent_review_node)
workflow.add_node("query_database", query_database_node)
workflow.add_node("generate_config", generate_config_node)
workflow.add_node("config_review", config_review_node)
workflow.add_node("post_to_engine", post_to_engine_node)
workflow.add_node("show_result", show_result_node)
workflow.add_node("error_handler", error_node)

# è®¾ç½®å…¥å£ç‚¹
workflow.set_entry_point("intent_recognition")

# æ·»åŠ è¾¹
workflow.add_edge("intent_recognition", "intent_review")
workflow.add_edge("query_database", "generate_config")
workflow.add_edge("generate_config", "config_review")
workflow.add_edge("post_to_engine", "show_result")

# æ·»åŠ æ¡ä»¶è¾¹
workflow.add_conditional_edges(
    "intent_review",
    should_continue_after_intent_review,
    {
        "continue": "query_database",
        "revise_intent": "intent_recognition",
        "error": "error_handler",
    },
)

workflow.add_conditional_edges(
    "config_review",
    should_continue_after_config_review,
    {
        "continue": "post_to_engine",
        "revise_config": "generate_config",
        "error": "error_handler",
    },
)

# è¿æ¥åˆ°ç»ˆç‚¹
workflow.add_edge("show_result", END)
workflow.add_edge("error_handler", END)

# ç¼–è¯‘Graph
app = workflow.compile()


# --- 7. Chainlit UIäº¤äº’ ---


@cl.on_chat_start
async def start_chat():
    cl.user_session.set("graph_runner", app)
    await cl.Message(
        content="æ‚¨å¥½ï¼æˆ‘æ˜¯æ‚¨çš„æµ‹è¯•æ•°æ®ç”ŸæˆåŠ©æ‰‹ã€‚\n\n"
        "è¯·å‘Šè¯‰æˆ‘æ‚¨æƒ³ä¸ºä»€ä¹ˆè¡¨ç”Ÿæˆæ•°æ®ï¼Œä»¥åŠå…·ä½“çš„è¦æ±‚ã€‚\n\n"
        "**ä¾‹å¦‚ï¼š**\n"
        "æˆ‘æƒ³ä¸º `users` è¡¨ç”Ÿæˆ100æ¡ç”¨æˆ·æ•°æ®ï¼Œè¦æ±‚å¹´é¾„åœ¨18åˆ°40å²ä¹‹é—´ï¼Œé‚®ç®±å¿…é¡»æ˜¯gmailåç¼€ã€‚"
    ).send()


@cl.on_message
async def on_message(message: cl.Message):
    graph_runner = cl.user_session.get("graph_runner")

    inputs = {"user_initial_intent": message.content}

    # ainvoke_stream åœ¨è¿™é‡Œå¯ä»¥å®æ—¶æ˜¾ç¤ºæ¯ä¸€æ­¥çš„çŠ¶æ€
    async for output in graph_runner.astream(inputs, {"recursion_limit": 20}):
        # astream() returns states after each step is executed
        # We can inspect the state and display information to the user
        pass
